{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734522cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "\n",
    "file_path = r\"D:\\Project\\web_mining\\notebooks\\ratings_test_clean.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def parse_list(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(value)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except:\n",
    "            return None\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    return None\n",
    "\n",
    "unique_counts = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    unique_values = set()\n",
    "\n",
    "    for val in df[col]:\n",
    "        lst = parse_list(val)\n",
    "        if lst:\n",
    "            for item in lst:\n",
    "                # Tránh list lồng\n",
    "                if isinstance(item, list):\n",
    "                    for sub in item:\n",
    "                        unique_values.add(str(sub))\n",
    "                else:\n",
    "                    unique_values.add(str(item))\n",
    "\n",
    "    # Chỉ lưu cột có dữ liệu dạng list\n",
    "    if unique_values:\n",
    "        unique_counts[col] = len(unique_values)\n",
    "\n",
    "# In kết quả\n",
    "for col, cnt in unique_counts.items():\n",
    "    print(f\"{col}: {cnt}\")\n",
    "\n",
    "def parse_vector(data_str):\n",
    "    data_array = np.fromstring(data_str.strip(\"[]\"), sep=' ')\n",
    "    data_list = data_array.tolist()\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a96202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def safe_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return set(x)\n",
    "    return set()\n",
    "\n",
    "def jaccard(a, b):\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return len(a & b) / len(a | b)\n",
    "\n",
    "def cosine_sim(vec1, vec2, eps=1e-8):\n",
    "    vec1 = np.asarray(vec1)\n",
    "    vec2 = np.asarray(vec2)\n",
    "\n",
    "    return np.dot(vec1, vec2) / (\n",
    "        np.linalg.norm(vec1) * np.linalg.norm(vec2) + eps\n",
    "    )\n",
    "\n",
    "def compute_similarity(metric, v1, v2):\n",
    "    if metric == \"jaccard\":\n",
    "        return jaccard(safe_list(v1), safe_list(v2))\n",
    "    elif metric == \"cosine\":\n",
    "        return cosine_sim(v1, v2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "def movie_similarity(m1, m2, sim_params):\n",
    "    score = 0.0\n",
    "\n",
    "    for field, cfg in sim_params.items():\n",
    "        weight = cfg[\"weight\"]\n",
    "        metric = cfg[\"metric\"]\n",
    "\n",
    "        val1 = m1.get(field)\n",
    "        val2 = m2.get(field)\n",
    "\n",
    "        if val1 is None or val2 is None:\n",
    "            continue\n",
    "        \n",
    "        sim = compute_similarity(metric, val1, val2)\n",
    "        score += weight * sim\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbec786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user(user_id, movie_obj, train_df, sim_params, K=5):\n",
    "    \"\"\"\n",
    "    user_id: int\n",
    "    movie_obj: dict (1 movie)\n",
    "    train_df: DataFrame (ratings_train merged metadata)\n",
    "    sim_params: dict (similarity parameters)\n",
    "    \"\"\"\n",
    "    user_movies = train_df[train_df[\"userId\"] == user_id]\n",
    "    if user_movies.empty:\n",
    "        return 0.0\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for _, row in user_movies.iterrows():\n",
    "        for field in [\"keyword_names_bert\", \"overview_bert\"]:\n",
    "            row[field] = parse_vector(row[field])\n",
    "        sim = movie_similarity(movie_obj, row, sim_params)\n",
    "        scores.append((row[\"movieId\"], row[\"rating\"], sim))\n",
    "\n",
    "    scores.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    top_k = scores[:K]\n",
    "\n",
    "    avg_rating = sum(r[1] for r in top_k) / len(top_k)\n",
    "    return avg_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from functools import partial\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(r\"D:\\Project\\web_mining\\notebooks\\ratings_train_clean_bert.csv\")\n",
    "    test_df = pd.read_csv(r\"D:\\Project\\web_mining\\notebooks\\ratings_test_clean_bert.csv\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    train_df = pd.DataFrame() \n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    weights = {\n",
    "        {'genres': {'metric': 'jaccard', 'weight': 0.07695564868227772}, 'keyword_names': {'metric': 'jaccard', 'weight': 0.20469569241159363}, 'top_cast': {'metric': 'jaccard', 'weight': 0.8330251175685949}, 'writers': {'metric': 'jaccard', 'weight': 0.17213970919049904}, 'producers': {'metric': 'jaccard', 'weight': 0.3575349111578864}, 'belongs_to_collection': {'metric': 'jaccard', 'weight': 0.023259311007562167}, 'keyword_names_bert': {'metric': 'cosine', 'weight': 0.5896342359730261}, 'overview_bert': {'metric': 'cosine', 'weight': 0.8317850183322544}}\n",
    "    }\n",
    "\n",
    "    MOVIE_FIELDS = [\n",
    "        \"genres\", \"keyword_names\", \"top_cast\", \"writers\", \n",
    "        \"producers\", \"belongs_to_collection\", \"keyword_names_bert\", \"overview_bert\"\n",
    "    ]\n",
    "    \n",
    "    K = 5\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        movie = {f: row[f] for f in MOVIE_FIELDS}\n",
    "        \n",
    "        for field in [\"keyword_names_bert\", \"overview_bert\"]:\n",
    "            movie[field] = parse_vector(movie[field])\n",
    "            \n",
    "        try:\n",
    "            pred_rating = recommend_for_user(row[\"userId\"], movie, train_df, weights, K)\n",
    "        except NameError:\n",
    "            pred_rating = 0.0 \n",
    "        \n",
    "        y_true.append(row[\"rating\"])\n",
    "        y_pred.append(pred_rating)\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "\n",
    "\"\"\"if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    print(\"Bắt đầu tìm kiếm tham số tối ưu...\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Best MSE: {study.best_value}\")\n",
    "    print(\"Best Params:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    best_sim_params = {}\n",
    "    metrics_map = { \n",
    "        \"w_genres\": (\"genres\", \"jaccard\"),\n",
    "        \"w_keyword\": (\"keyword_names\", \"jaccard\"),\n",
    "        \"w_cast\": (\"top_cast\", \"jaccard\"),\n",
    "        \"w_writers\": (\"writers\", \"jaccard\"),\n",
    "        \"w_producers\": (\"producers\", \"jaccard\"),\n",
    "        \"w_collection\": (\"belongs_to_collection\", \"jaccard\"),\n",
    "        \"w_key_bert\": (\"keyword_names_bert\", \"cosine\"),\n",
    "        \"w_overview_bert\": (\"overview_bert\", \"cosine\")\n",
    "    }\n",
    "    \n",
    "    for param_name, weight_val in study.best_params.items():\n",
    "        field, metric = metrics_map[param_name]\n",
    "        best_sim_params[field] = {\"metric\": metric, \"weight\": weight_val}\n",
    "        \n",
    "    print(best_sim_params)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3eefdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(r\"D:\\Project\\web_mining\\notebooks\\ratings_train_clean_bert.csv\")\n",
    "df_test  = pd.read_csv(r\"D:\\Project\\web_mining\\notebooks\\ratings_test_clean_bert.csv\")\n",
    "\n",
    "all_users = pd.concat([df_train[\"userId\"], df_test[\"userId\"]]).unique()\n",
    "all_items = pd.concat([df_train[\"movieId\"], df_test[\"movieId\"]]).unique()\n",
    "\n",
    "user2idx = {u: i for i, u in enumerate(all_users)}\n",
    "item2idx = {i: j for j, i in enumerate(all_items)}\n",
    "\n",
    "idx2user = {i: u for u, i in user2idx.items()}\n",
    "idx2item = {j: i for i, j in item2idx.items()}\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "train_interactions = defaultdict(set)\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "    u = user2idx[row[\"userId\"]]\n",
    "    i = item2idx[row[\"movieId\"]]\n",
    "    train_interactions[u].add(i)\n",
    "\n",
    "test_ratings = defaultdict(dict)\n",
    "\n",
    "\n",
    "MOVIE_FIELDS = [\n",
    "        \"genres\", \"keyword_names\", \"top_cast\", \"writers\", \n",
    "        \"p\"\n",
    "        \"roducers\", \"belongs_to_collection\", \"keyword_names_bert\", \"overview_bert\"\n",
    "    ]\n",
    "weights = {'genres': {'metric': 'jaccard', 'weight': 0.07695564868227772}, 'keyword_names': {'metric': 'jaccard', 'weight': 0.20469569241159363}, 'top_cast': {'metric': 'jaccard', 'weight': 0.8330251175685949}, 'writers': {'metric': 'jaccard', 'weight': 0.17213970919049904}, 'producers': {'metric': 'jaccard', 'weight': 0.3575349111578864}, 'belongs_to_collection': {'metric': 'jaccard', 'weight': 0.023259311007562167}, 'keyword_names_bert': {'metric': 'cosine', 'weight': 0.5896342359730261}, 'overview_bert': {'metric': 'cosine', 'weight': 0.8317850183322544}}\n",
    "num_users = len(user2idx)\n",
    "num_items = len(item2idx)\n",
    "ratings_pred = np.zeros((num_users, num_items))\n",
    "for _, row in df_test.iterrows():\n",
    "    u = user2idx[row[\"userId\"]]\n",
    "    i = item2idx[row[\"movieId\"]]\n",
    "    test_ratings[u][i] = float(row[\"rating\"])\n",
    "\n",
    "    movie_obj = {f: row[f] for f in MOVIE_FIELDS}\n",
    "    for field in [\"keyword_names_bert\", \"overview_bert\"]:\n",
    "        movie_obj[field] = parse_vector(movie_obj[field])\n",
    "    ratings_pred[u][i] = recommend_for_user(row[\"userId\"], movie_obj, df_train, weights, K=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b779def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr_at_k(\n",
    "    ratings_pred,\n",
    "    test_ratings,\n",
    "    rating_threshold=3.5,\n",
    "    k=5\n",
    "):\n",
    "\n",
    "    rr_scores = []\n",
    "\n",
    "    for u in range(ratings_pred.shape[0]):\n",
    "        if u not in test_ratings:\n",
    "            continue\n",
    "\n",
    "        # top-k theo rating dự đoán\n",
    "        top_k_items = np.argsort(-ratings_pred[u])[:k]\n",
    "\n",
    "        rr_u = 0.0\n",
    "        for rank, item_id in enumerate(top_k_items, start=1):\n",
    "            if (\n",
    "                item_id in test_ratings[u]\n",
    "                and test_ratings[u][item_id] >= rating_threshold\n",
    "            ):\n",
    "                rr_u = 1.0 / rank\n",
    "                break   # chỉ lấy item relevant đầu tiên\n",
    "\n",
    "        rr_scores.append(rr_u)\n",
    "\n",
    "    return float(np.mean(rr_scores)) if rr_scores else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1492f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hr_at_k(\n",
    "    ratings_pred,\n",
    "    test_ratings,\n",
    "    rating_threshold=3.5,\n",
    "    k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    ratings_pred: np.ndarray (num_users, num_items)\n",
    "    test_ratings: dict {u: {item_id: rating}}\n",
    "    \"\"\"\n",
    "\n",
    "    hits = []\n",
    "\n",
    "    for u in range(ratings_pred.shape[0]):\n",
    "        if u not in test_ratings:\n",
    "            continue\n",
    "\n",
    "        # top-k items\n",
    "        top_k_items = np.argsort(-ratings_pred[u])[:k]\n",
    "\n",
    "        hit_u = 0\n",
    "        for item_id in top_k_items:\n",
    "            if (\n",
    "                item_id in test_ratings[u]\n",
    "                and test_ratings[u][item_id] >= rating_threshold\n",
    "            ):\n",
    "                hit_u = 1\n",
    "                break\n",
    "\n",
    "        hits.append(hit_u)\n",
    "\n",
    "    return float(np.mean(hits)) if hits else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00e03010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@5 = 0.7678463855421687\n"
     ]
    }
   ],
   "source": [
    "mrr5 = compute_mrr_at_k(\n",
    "    ratings_pred=ratings_pred,\n",
    "    test_ratings=test_ratings,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"MRR@5 =\", mrr5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf2dc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@5 = 0.9367469879518072\n"
     ]
    }
   ],
   "source": [
    "hr = compute_hr_at_k(\n",
    "    ratings_pred=ratings_pred,\n",
    "    test_ratings=test_ratings,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"HR@5 =\", hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e12f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts = Counter(df_train[\"userId\"])\n",
    "sorted_users = sorted(user_counts.items(), key=lambda x: x[1])\n",
    "least_5_users_raw = [u for u, _ in sorted_users[:5]]\n",
    "most_5_users_raw = [u for u, _ in sorted_users[-5:]]\n",
    "least_5_users = [user2idx[u] for u in least_5_users_raw if u in user2idx]\n",
    "most_5_users  = [user2idx[u] for u in most_5_users_raw if u in user2idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f958f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_user_mse(u, ratings_pred, test_ratings):\n",
    "    if u not in test_ratings:\n",
    "        return None\n",
    "\n",
    "    errors = []\n",
    "    for i, true_rating in test_ratings[u].items():\n",
    "        pred = ratings_pred[u][i]\n",
    "        errors.append((pred - true_rating) ** 2)\n",
    "\n",
    "    return np.mean(errors) if errors else None\n",
    "\n",
    "def compute_user_mrr_at_k(\n",
    "    u,\n",
    "    ratings_pred,\n",
    "    test_ratings,\n",
    "    rating_threshold=3.5,\n",
    "    k=5\n",
    "):\n",
    "    if u not in test_ratings:\n",
    "        return 0.0\n",
    "\n",
    "    top_k_items = np.argsort(-ratings_pred[u])[:k]\n",
    "\n",
    "    for rank, i in enumerate(top_k_items, start=1):\n",
    "        if i in test_ratings[u] and test_ratings[u][i] >= rating_threshold:\n",
    "            return 1.0 / rank\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "def compute_user_hr_at_k(\n",
    "    u,\n",
    "    ratings_pred,\n",
    "    test_ratings,\n",
    "    rating_threshold=3.5,\n",
    "    k=5\n",
    "):\n",
    "    if u not in test_ratings:\n",
    "        return 0.0\n",
    "\n",
    "    top_k_items = np.argsort(-ratings_pred[u])[:k]\n",
    "\n",
    "    for i in top_k_items:\n",
    "        if i in test_ratings[u] and test_ratings[u][i] >= rating_threshold:\n",
    "            return 1.0 \n",
    "\n",
    "    return 0.0     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdf2ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_user_group(\n",
    "    users,\n",
    "    ratings_pred,\n",
    "    test_ratings\n",
    "):\n",
    "    mses = []\n",
    "    hrs = []\n",
    "    mrrs  = []\n",
    "\n",
    "    for u in users:\n",
    "        mse = compute_user_mse(u, ratings_pred, test_ratings)\n",
    "        if mse is not None:\n",
    "            mses.append(mse)\n",
    "\n",
    "        hrs.append(\n",
    "            compute_user_hr_at_k(u, ratings_pred, test_ratings)\n",
    "        )\n",
    "\n",
    "        mrrs.append(\n",
    "            compute_user_mrr_at_k(u, ratings_pred, test_ratings)\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"Mean MSE\": float(np.mean(mses)) if mses else None,\n",
    "        \"Mean HR@5\": float(np.mean(hrs)) if hrs else 0.0,\n",
    "        \"MRR@5\": float(np.mean(mrrs)) if mrrs else 0.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a35b452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 5 USERS NHIỀU DATA NHẤT =====\n",
      "{'Mean MSE': 1.1941419687275943, 'Mean HR@5': 1.0, 'MRR@5': 0.5666666666666667}\n",
      "\n",
      "===== 5 USERS ÍT DATA NHẤT =====\n",
      "{'Mean MSE': 0.92578125, 'Mean HR@5': 0.6, 'MRR@5': 0.5}\n"
     ]
    }
   ],
   "source": [
    "results_most = evaluate_user_group(\n",
    "    most_5_users,\n",
    "    ratings_pred,\n",
    "    test_ratings\n",
    ")\n",
    "\n",
    "results_least = evaluate_user_group(\n",
    "    least_5_users,\n",
    "    ratings_pred,\n",
    "    test_ratings\n",
    ")\n",
    "\n",
    "print(\"===== 5 USERS NHIỀU DATA NHẤT =====\")\n",
    "print(results_most)\n",
    "\n",
    "print(\"\\n===== 5 USERS ÍT DATA NHẤT =====\")\n",
    "print(results_least)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
